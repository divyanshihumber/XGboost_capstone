# Capstone Project â€“ XGBoost: An Optimized Boosting Algorithm

This repository contains the implementation, code, and supporting files for our Capstone Project on **XGBoost**, completed as part of the Machine Learning (RNA) course.

## ğŸ“Œ Project Overview

XGBoost (Extreme Gradient Boosting) is a powerful ensemble learning algorithm known for its speed and performance. This project explores how XGBoost works, compares it with Random Forest, and demonstrates its application on a real-world classification dataset.

## ğŸ“ Contents

- `xgboost_model.py` â€“ Python code for training and evaluating the XGBoost model.
- `data/` â€“ Contains the dataset used (or link if using sklearn datasets).
- `notebooks/` â€“ Jupyter notebooks for data preprocessing, training, and visualization.
- `presentation/` â€“ Final presentation slides (`.pptx`) for the project.
- `report.pdf` â€“ Written report submitted for evaluation.

## ğŸš€ How to Run

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/xgboost-capstone.git
   cd xgboost-capstone
2. Install dependencies:
pip install -r requirements.txt

3. Run the main script from the jupyter notebook capstone.ipynb


**Visualizations**
ROC Curve

Confusion Matrix

Feature Importance


**Technologies Used**
Python

XGBoost

Scikit-learn

Matplotlib / Seaborn

Pandas / NumPy

**Authors**
Divyanshi

Arif Elshafea

Rabiya Malik

**Course Info**
Course: Machine Learning (RNA)
Instructor: Caryn Geady
Date: July 24, 2025
